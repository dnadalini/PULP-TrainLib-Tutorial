# TrainLib Deployer: code generation for you On-Device Learning project

NOTE: THE WAY TO GENERATE THE NETWORK SHOULD BE CHANGED (CRISTI + ME)

This first exercise aims at showing you how you can generate and edit the code for On-Device Learning on Convolutional Neural Networks with PULP-TrainLib.
Here, we will modify the `USER SECTION` of [TrainLib_Deployer](../pulp-trainlib/tools/TrainLib_Deployer/TrainLib_Deployer.py) to generate the code for a CNN, deployed with FP32 computations.

## Generate a FP32 CNN

To generate this example, first overwrite the corresponding lines of [TrainLib_Deployer](../pulp-trainlib/tools/TrainLib_Deployer/TrainLib_Deployer.py) with the content of [CNN_FP32.txt](CNN_FP32.txt). Then, open a new terminal in this repository root folder (PULP-TrainLib-Tutorial/) and:

```
cd ../pulp-trainlib/tools/TrainLib_Deployer
python TrainLib_Deployer.py
```

This will generate the code in `Ex01-TrainLib_Deployer/CNN_FP32/`. Then:

```
source setup.sh
cd ../../../Ex01-TrainLib_Deployer/CNN_FP32/
make clean get_golden all run
```

Executing the last command will generate the golden model from PyTorch (`make get_golden`) and compile the code for the execution on the PULP simulator (`make clean all run`). On the terminal, you will find both the functional and profiling information.

```
make clean         : deletes executable
make get_golden    : generates the Golden Model's reference files
make all           : compiles the C code application
make run           : executes the code on 
```

The typical output of the generated example will be like:

```
TYPICAL OUTPUT OF AN APPLICATION
```

## On-Device Learning Code: net.c

TrainLib_Deployer generates the On-Device Learning code in form of a set of functions and data definitions included into [net.c](). Configuration options are included in the [Makefile](). This code is intended to work as a starting point for your application.

The reference data populating the weight and activation tensors is generated by [GM.py]() - the Golden Model or reference - into several files ([io-data.h](), [params.h]()), which are included by [net.c]().

The structure of a training flow is managed by the following functions:

```C
void forward() {
    // First layer's primitive

    // Second layer's primitive

    // ...

    // Last layer's primitive

}

void backward() {


}

...
```

Data generated by the Golden Model is provided to each layer's primitives in the following way (example follows):

```C
// Give the example of a fully-connected layer

PI_L1 float data[];
PI_L1 float gradient[];
PI_L1 int size;

// Define blob

// Define configuration structure

```

## Parallelizing over 8 Cores

The code that was previously generated runs on a single core (`NUM_CORES=1`). To exploit the available Cluster cores (8 in this case), we need to modify the parameter `NUM_CORES=8` into the [Makefile]().

Then, we can recompile with `rm -rf BUILD/; make clean all run`.



