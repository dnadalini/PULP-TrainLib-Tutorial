# TrainLib Deployer: code generation for you On-Device Learning project

This first exercise aims at showing you how you can generate and edit the code for On-Device Learning on Convolutional Neural Networks with PULP-TrainLib.
Here, we will modify the `USER SECTION` of [TrainLib_Deployer](../pulp-trainlib/tools/TrainLib_Deployer/TrainLib_Deployer.py) to generate the code for a CNN, deployed with FP32 computations.

## Defining or importing your CNN

NOTE: THE WAY TO GENERATE THE NETWORK SHOULD BE CHANGED (CRISTI + ME)

To generate this example, first overwrite the corresponding lines of [TrainLib_Deployer](../pulp-trainlib/tools/TrainLib_Deployer/TrainLib_Deployer.py) with the content of [CNN_FP32.txt](CNN_FP32.txt).

In this tutorial, we will generate and validate a simple three-layer DNN, composed as follows:

![DNN](../img/DNN.png)

## Generate a FP32 CNN

With a terminal open in this repository root folder (PULP-TrainLib-Tutorial/) run the TrainLib_Deployer to generate the sample ODL code in a specified folder:

```
conda activate trainlib-tutorial
cd ../pulp-trainlib/tools/TrainLib_Deployer
python TrainLib_Deployer.py
```

This will generate the code in `Ex01-TrainLib_Deployer/CNN_FP32/`, as specified in the options. Then, setup the environment with:

```
cd ../../..
source setup.sh
```
Now, the terminal is ready to compile the generated code. Let's:

```
cd Ex01-TrainLib_Deployer/CNN_FP32/
make clean get_golden all run
```

Executing the last command will generate the golden model from PyTorch (`make get_golden`) and compile the code for the execution on the PULP simulator (`make clean all run`). On the terminal, you will find both the functional and profiling information.

```
make clean         : deletes executable
make get_golden    : generates the Golden Model's reference files
make all           : compiles the C code application
make run           : executes the code on 
```

The typical output of the generated example will be like: 

```
Hello sir.
Configuring cluster..

Launching training procedure...
Initializing network..
Testing DNN initialization forward..
Layer 2 output:        _
-0.000023               |
0.000048                |
-0.000030               |
0.000024                \
-0.000081                \  DNN Output before ODL
0.000009                 /  
0.000061                /
-0.000025               |
-0.000021               |
-0.000006              _|

                                --- STATISTICS FROM CLUSTER CORE 0  ---
[0] cycles = 736089             <= Cycles to execute the program (LATENCY)
[0] instr = 571415              <= Number of instructions
[0] active cycles = 736042      <= Number of active cycles
[0] ext load = 60               <= Direct loads from L2
[0] TCDM cont = 0               <= Memory contentions in L1
[0] ld stall = 150245           <= Stalls while loading data
[0] imiss = 2288                <= Instruction cache misses
Checking updated output..

Layer 2 output:        _
-0.000023               |
0.000048                |
-0.000030               |
0.000024                \
-0.000081                \  DNN Output AFTER ODL
0.000009                 / 
0.000061                /
-0.000025               |
-0.000021               |
-0.000006              _|
Exiting DNN Training.
```

## On-Device Learning Code: net.c

TrainLib_Deployer generates the On-Device Learning code in form of a set of functions and data definitions included into [net.c](). Configuration options are included in the [Makefile](). This code is intended to work as a starting point for your application.

The reference and initialization data populating the weight and activation tensors is generated by PyTorch-written [GM.py](CNN_FP32/utils/GM.py) - the Golden Model or reference - into several files ([io-data.h](CNN_FP32/io_data.h), [init-defines.h](CNN_FP32/init-defines.h)), which are included by [net.c](CNN_FP32/net.c).

The structure of a training flow is managed by the following functions:

```C
void forward() {
    // First layer's primitive
    pulp_conv2d_fp32_fw_cl(&l0_args);
    // Second layer's primitive
    pulp_relu_fp32_fw_cl(&l1_args);
    // Last layer's primitive
    pulp_linear_fp32_fw_cl(&l2_args);
}

void backward() {
    // Last layer's primitive
    pulp_linear_fp32_bw_cl(&l2_args);
    // Second layer's primitive
    pulp_relu_fp32_bw_cl(&l1_args);
    // First layer's primitive
    pulp_conv2d_fp32_bw_cl(&l0_args);
}
```

The loss function is managed by the `void compute_loss()` function, while the `void update_weights()` method takes care of updating the weights after each training iteration. 

Each layer is connected to the static arrays through the `blob` structure, which represents a tensor. For example, in case of a Fully-Connected Layer (as Layer 2):

```C
// Statically define data (C arrays)
PI_L1 float l2_in[Tin_C_l2 * Tin_H_l2 * Tin_W_l2];
PI_L1 float l2_in_diff[Tin_C_l2 * Tin_H_l2 * Tin_W_l2];
PI_L1 float l2_ker[Tin_C_l2 * Tout_C_l2 * Tker_H_l2 * Tker_W_l2];
PI_L1 float l2_ker_diff[Tin_C_l2 * Tout_C_l2 * Tker_H_l2 * Tker_W_l2];
PI_L1 float l2_out[Tout_C_l2 * Tout_H_l2 * Tout_W_l2];
PI_L1 float l2_out_diff[Tout_C_l2 * Tout_H_l2 * Tout_W_l2];

// Define the tensors as "blobs"
PI_L1 struct blob layer2_in, layer2_wgt, layer2_out;

// Define the arguments for the Fully-Connected
PI_L1 struct Linear_args l2_args;
```

Then, the static arrays are linked to the `blob` and the `arguments` at runtime, during the initialization:

```C
void DNN_init() {
    // ...

    // Example of one tensor definition for Layer 2
    layer0_wgt.data = l0_ker;
    layer0_wgt.diff = l0_ker_diff;
    layer0_wgt.dim = Tin_C_l0*Tout_C_l0*Tker_H_l0*Tker_W_l0;
    layer0_wgt.C = Tin_C_l0;
    layer0_wgt.H = Tker_H_l0;
    layer0_wgt.W = Tker_W_l0;    

    // ...

    // Configuration structure for Layer 2
    l2_args.input = &layer2_in;
    l2_args.coeff = &layer2_wgt;
    l2_args.output = &layer2_out;
    l2_args.skip_in_grad = 0;
    l2_args.opt_matmul_type_fw = MATMUL_TYPE_FW_L2;
    l2_args.opt_matmul_type_wg = MATMUL_TYPE_WG_L2;
    l2_args.opt_matmul_type_ig = MATMUL_TYPE_IG_L2;
}
```

Now, the layer is ready to perform both forward and backward steps.

The training routine is triggered by the void `net_step()` function, which essentially performs the following routine:

```C
void net_step()
{
  DNN_init();
  forward();

  for (int epoch=0; epoch<EPOCHS; epoch++)
  {
    forward();
    compute_loss();
    backward();
    update_weights();
  }
}
```


## Parallelizing over 8 Cores

The code that was previously generated runs on a single core (`NUM_CORES=1`). We can exploit the available Cluster cores (8 in this case) by modifying the parameter `NUM_CORES=8` into the [Makefile](CNN_FP32/Makefile) accordingly.

Then, the code can be recompiled with:

```
rm -rf BUILD/; make clean all run
``` 

Parallelizing over multiple cores is managed by the function `pi_cl_team_fork(NUM_CORES, parallel_function, &args)`, which is widely used in PULP-TrainLib's kernels.

Let's consider the case of a Fully-Connected Layer, where the core computational kernels can be expressed as Matrix Multiplication. Let's consider, for example, the Forward step:

![FC_Forward](../img/FC_forward.png)

One way to exploit parallelism is to distribute the computation of only 1/Nth of the rows the first matrix - i.e., the weights - to each core. Therefore, the computation of the output matrix - the layer's output - is split in different chunks that are obtained by each core.

In this case, parallelization is not ideal, due to the limited sizes of the layers. However, the total cycles can be reduced by 4.65 times:

```
[0] cycles = 736089     <= Cycles with 1 Core
[0] cycles = 158376     <= Cycles with 8 Cores
PARALLEL SPEEDUP: 736089 / 158376 = 4.65
``` 



